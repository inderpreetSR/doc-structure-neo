<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Pipeline 3: RAG-Based Knowledge Base</title>
  <meta name="description" content="Pipeline 3: Embed your codebase into a vector database and query it with retrieval-augmented generation.">
  <link rel="stylesheet" href="assets/style.css">
</head>
<body>

  <a class="skip-link" href="#main">Skip to content</a>

  <header>
    <nav class="top-nav" aria-label="Primary">
      <span class="logo">DocPipelines</span>
      <a href="index.html">Home</a>
      <a href="1-code-aware.html">Code-Aware</a>
      <a href="2-git-driven.html">Git-Driven</a>
      <a href="3-rag-knowledge.html" class="active" aria-current="page">RAG</a>
      <a href="4-living-docs.html">Living Docs</a>
      <a href="5-multi-source.html">Multi-Source</a>
      <a href="6-full-stack.html">Full-Stack</a>
    </nav>
  </header>

  <main id="main">
    <section class="hero">
      <h1>RAG-Based Knowledge Base</h1>
      <p>Embed your entire codebase into a vector database and let developers query documentation in natural language with retrieval-augmented generation.</p>
    </section>

    <section class="page-content">

    <h2>Architecture</h2>
    <div class="flow">
      <span class="step">Codebase</span>
      <span class="arrow">&rarr;</span>
      <span class="step">Chunker</span>
      <span class="arrow">&rarr;</span>
      <span class="step">Embeddings</span>
      <span class="arrow">&rarr;</span>
      <span class="step">Vector DB</span>
      <span class="arrow">&rarr;</span>
      <span class="step">Query API</span>
      <span class="arrow">&rarr;</span>
      <span class="step">LLM Answer</span>
    </div>

    <h2>How It Works</h2>
    <ol>
      <li><strong>Chunk</strong> &mdash; Split source files into semantic chunks (functions, classes, logical blocks)</li>
      <li><strong>Embed</strong> &mdash; Generate vector embeddings using OpenAI, Sentence Transformers, or Cohere</li>
      <li><strong>Store</strong> &mdash; Insert embeddings into ChromaDB, Pinecone, or Qdrant</li>
      <li><strong>Query</strong> &mdash; User asks a question in natural language</li>
      <li><strong>Retrieve</strong> &mdash; Find the top-K most relevant chunks via similarity search</li>
      <li><strong>Generate</strong> &mdash; Feed retrieved chunks + question to an LLM for a synthesized answer</li>
    </ol>

    <h2>Pipeline Script</h2>
    <pre><code># pipelines/3-rag-knowledge/pipeline.py
# Requirements: pip install chromadb openai tiktoken
# Run: python pipeline.py --source ./your-project

import os, json, hashlib, argparse, textwrap
from pathlib import Path
from datetime import datetime

# ---- Chunking ----

class CodeChunker:
    """Split source files into meaningful chunks."""

    def __init__(self, max_chunk_size=1500):
        self.max_chunk_size = max_chunk_size

    def chunk_file(self, filepath):
        """Chunk a file by functions/classes or by line groups."""
        source = filepath.read_text(encoding='utf-8', errors='ignore')
        lines = source.splitlines()
        chunks = []
        current_chunk = []
        current_size = 0

        for line in lines:
            current_chunk.append(line)
            current_size += len(line)
            # Split on function/class boundaries or size limit
            if (current_size >= self.max_chunk_size and
                (line.strip() == '' or line.startswith('def ') or
                 line.startswith('class '))):
                chunks.append('\n'.join(current_chunk))
                current_chunk = []
                current_size = 0

        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        return chunks

# ---- Vector Store (ChromaDB) ----

class VectorStore:
    """Manage embeddings in ChromaDB."""

    def __init__(self, db_path='./chroma_db', collection_name='codebase'):
        try:
            import chromadb
            self.client = chromadb.PersistentClient(path=db_path)
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            self.available = True
        except ImportError:
            print("[warn] chromadb not installed, using in-memory fallback")
            self.available = False
            self.memory_store = []

    def add_chunks(self, chunks, metadatas, ids):
        if self.available:
            self.collection.upsert(
                documents=chunks,
                metadatas=metadatas,
                ids=ids
            )
        else:
            for chunk, meta, id_ in zip(chunks, metadatas, ids):
                self.memory_store.append({
                    'id': id_, 'text': chunk, 'metadata': meta
                })

    def query(self, question, n_results=5):
        if self.available:
            results = self.collection.query(
                query_texts=[question],
                n_results=n_results
            )
            return results
        else:
            # Simple keyword matching fallback
            scored = []
            words = question.lower().split()
            for item in self.memory_store:
                score = sum(1 for w in words if w in item['text'].lower())
                scored.append((score, item))
            scored.sort(key=lambda x: -x[0])
            return {
                'documents': [[s[1]['text'] for s in scored[:n_results]]],
                'metadatas': [[s[1]['metadata'] for s in scored[:n_results]]]
            }

    def count(self):
        if self.available:
            return self.collection.count()
        return len(self.memory_store)

# ---- RAG Pipeline ----

class RAGDocPipeline:
    """Full RAG pipeline for codebase documentation."""

    def __init__(self, source_dir, db_path='./chroma_db'):
        self.source_dir = Path(source_dir)
        self.chunker = CodeChunker()
        self.store = VectorStore(db_path)

    def ingest(self, extensions=('.py', '.js', '.ts', '.md', '.yaml', '.json')):
        """Ingest all source files into the vector store."""
        files = []
        for ext in extensions:
            files.extend(self.source_dir.rglob(f'*{ext}'))

        total_chunks = 0
        for filepath in sorted(files):
            if '.git' in str(filepath) or 'node_modules' in str(filepath):
                continue
            rel = str(filepath.relative_to(self.source_dir))
            chunks = self.chunker.chunk_file(filepath)

            ids = [hashlib.md5(f"{rel}:{i}".encode()).hexdigest()
                   for i in range(len(chunks))]
            metadatas = [{
                'source': rel,
                'chunk_index': i,
                'language': filepath.suffix.lstrip('.'),
                'timestamp': datetime.now().isoformat()
            } for i in range(len(chunks))]

            self.store.add_chunks(chunks, metadatas, ids)
            total_chunks += len(chunks)
            print(f"[ingest] {rel} -> {len(chunks)} chunks")

        print(f"\n[done] Ingested {total_chunks} chunks from {len(files)} files")
        print(f"[done] Vector store contains {self.store.count()} entries")

    def query(self, question, n_results=5):
        """Query the knowledge base."""
        results = self.store.query(question, n_results)
        context_parts = []
        sources = []
        for doc, meta in zip(results['documents'][0], results['metadatas'][0]):
            context_parts.append(doc)
            sources.append(meta.get('source', 'unknown'))

        context = '\n\n---\n\n'.join(context_parts)

        # LLM prompt (replace with actual API call)
        llm_prompt = textwrap.dedent(f"""
        Based on the following code context, answer the question.

        CONTEXT:
        {context}

        QUESTION: {question}

        Provide a clear, detailed answer with code references.
        """)

        return {
            'question': question,
            'context_chunks': len(context_parts),
            'sources': list(set(sources)),
            'llm_prompt': llm_prompt,
            'answer': f"[LLM would answer based on {len(context_parts)} relevant chunks from: {', '.join(set(sources))}]"
        }

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='RAG Documentation Pipeline')
    parser.add_argument('--source', required=True, help='Source directory to ingest')
    parser.add_argument('--query', help='Question to ask (skip for ingest-only)')
    parser.add_argument('--db', default='./chroma_db', help='ChromaDB path')
    args = parser.parse_args()

    pipeline = RAGDocPipeline(args.source, args.db)
    pipeline.ingest()

    if args.query:
        result = pipeline.query(args.query)
        print(f"\nQuestion: {result['question']}")
        print(f"Sources: {result['sources']}")
        print(f"Answer: {result['answer']}")
</code></pre>

    <h2>Key Features</h2>
    <ul>
      <li>Semantic code chunking by function/class boundaries</li>
      <li>ChromaDB vector storage with cosine similarity</li>
      <li>Fallback in-memory keyword search when ChromaDB unavailable</li>
      <li>Multi-language support (Python, JS, TS, Markdown, YAML)</li>
      <li>Ready for OpenAI / Claude / local LLM integration</li>
      <li>Incremental updates via content-hash based IDs</li>
    </ul>

    <h2>Usage</h2>
    <pre><code># Ingest codebase
python pipeline.py --source ./your-project

# Ingest + query
python pipeline.py --source ./your-project --query "How does authentication work?"</code></pre>

    <h3>Requirements</h3>
    <pre><code>pip install chromadb openai tiktoken</code></pre>

    </section>
  </main>

  <footer>
    <a href="index.html" class="btn btn-outline" style="margin-bottom:1rem;">Back to All Pipelines</a>
    <p>&copy; 2026 Intelligent Documentation Pipelines</p>
  </footer>

</body>
</html>
